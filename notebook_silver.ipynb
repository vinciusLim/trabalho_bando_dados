{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook camada Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criar uma sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"Notebook_Silver\").getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import col, regexp_replace, upper, lpad\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para tratar Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para tratar o zip code e garantir que tenha 5 dígitos\n",
    "def tratar_zip_code(df, column_name):\n",
    "    return df.withColumn(\n",
    "        column_name, \n",
    "        lpad(col(column_name), 5, \"0\").cast(StringType())  # Preenche com 0 à esquerda se necessário\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, regexp_replace\n",
    "\n",
    "def tratamento_dados(df, columns):\n",
    "    \"\"\"\n",
    "    Remove acentos e pontuações de uma lista de colunas de texto em um DataFrame do PySpark\n",
    "    e substitui underscores por espaços.\n",
    "\n",
    "    Parâmetros:\n",
    "        df (DataFrame): O DataFrame contendo as colunas a serem tratadas.\n",
    "        columns (list): Lista de nomes das colunas a serem processadas.\n",
    "\n",
    "    Retorna:\n",
    "        DataFrame: O DataFrame com as colunas tratadas.\n",
    "    \"\"\"\n",
    "    for column_name in columns:\n",
    "        # Converter para maiúsculas\n",
    "        df = df.withColumn(column_name, upper(col(column_name)))\n",
    "\n",
    "        # Remover acentos\n",
    "        df = df.withColumn(\n",
    "            column_name,\n",
    "            regexp_replace(\n",
    "                col(column_name),\n",
    "                \"[áàâãäéèêëíìîïóòôõöúùûüç]\",  # Caracteres acentuados\n",
    "                \"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Substituir underscores por espaços\n",
    "        df = df.withColumn(\n",
    "            column_name,\n",
    "            regexp_replace(\n",
    "                col(column_name),\n",
    "                \"_\",  # Substitui underscore por espaço\n",
    "                \" \"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Remover pontuações\n",
    "        df = df.withColumn(\n",
    "            column_name,\n",
    "            regexp_replace(\n",
    "                col(column_name),\n",
    "                \"[^a-zA-Z0-9\\\\s]\",  # Remove tudo que não for letra, número ou espaço\n",
    "                \"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camadas de Origem e Destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "camada_origem = \"bronze\"\n",
    "camada_destino = \"silver\"\n",
    "os.makedirs(camada_origem, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando tabela customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/24 11:35:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extraindo tabela da Bronze\n",
    "customers = spark.read.parquet(\"bronze/customers/\")\n",
    "\n",
    "# Tratando colunas\n",
    "customers = tratamento_dados(customers, [\"customer_city\", \"customer_state\"])\n",
    "\n",
    "# Aplicando a função para tratar o zip code\n",
    "customers = tratar_zip_code(customers, \"customer_zip_code_prefix\")\n",
    "\n",
    "# Convertendo colunas para os tipos corretos\n",
    "customers = (\n",
    "    customers\n",
    "    .withColumn(\"customer_id\", col(\"customer_id\").cast(StringType()))\n",
    "    .withColumn(\"customer_unique_id\", col(\"customer_unique_id\").cast(StringType()))\n",
    "    .withColumn(\"customer_zip_code_prefix\", col(\"customer_zip_code_prefix\").cast(StringType()))\n",
    "    .withColumn(\"customer_city\", col(\"customer_city\").cast(StringType()))\n",
    "    .withColumn(\"customer_state\", col(\"customer_state\").cast(StringType()))\n",
    ")\n",
    "\n",
    "# Selecionando explicitamente todas as colunas\n",
    "customers = customers.select(\n",
    "    \"customer_id\",\n",
    "    \"customer_unique_id\",\n",
    "    \"customer_zip_code_prefix\",\n",
    "    \"customer_city\",\n",
    "    \"customer_state\"\n",
    ")\n",
    "\n",
    "# Remove linhas duplicadas\n",
    "customers = customers.dropDuplicates()\n",
    "\n",
    "# Salvando na Camada\n",
    "customers.write.mode(\"overwrite\").parquet(f\"{camada_destino}/customers/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando tabela geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/24 11:35:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/24 11:35:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/24 11:35:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/24 11:35:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/24 11:35:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/02/24 11:35:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/24 11:35:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/24 11:35:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/24 11:35:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extraindo tabela da Bronze\n",
    "geolocation = spark.read.parquet(\"bronze/geolocation/\")\n",
    "\n",
    "# Tratando colunas\n",
    "geolocation = tratamento_dados(geolocation, [\"geolocation_city\", \"geolocation_state\"])\n",
    "\n",
    "# Aplicando a função para tratar o zip code\n",
    "geolocation = tratar_zip_code(geolocation, \"geolocation_zip_code_prefix\")\n",
    "\n",
    "# Realizando o cast das colunas\n",
    "geolocation = (\n",
    "    geolocation\n",
    "    .withColumn(\"geolocation_zip_code_prefix\", col(\"geolocation_zip_code_prefix\").cast(StringType()))\n",
    "    .withColumn(\"geolocation_lat\", col(\"geolocation_lat\").cast(DoubleType()))\n",
    "    .withColumn(\"geolocation_lng\", col(\"geolocation_lng\").cast(DoubleType()))\n",
    "    .withColumn(\"geolocation_city\", col(\"geolocation_city\").cast(StringType()))\n",
    "    .withColumn(\"geolocation_state\", col(\"geolocation_state\").cast(StringType()))\n",
    ")\n",
    "\n",
    "# Selecionando as colunas necessárias\n",
    "geolocation = geolocation.select(\n",
    "    \"geolocation_zip_code_prefix\",\n",
    "    \"geolocation_lat\",\n",
    "    \"geolocation_lng\",\n",
    "    \"geolocation_city\",\n",
    "    \"geolocation_state\"\n",
    ")\n",
    "\n",
    "# Remove linhas duplicadas\n",
    "geolocation = geolocation.dropDuplicates()\n",
    "\n",
    "# Salvando na camada destino\n",
    "geolocation.write.mode(\"overwrite\").parquet(f\"{camada_destino}/geolocation/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando tabela Order Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/24 11:35:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/24 11:35:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/24 11:35:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/24 11:35:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/24 11:35:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/02/24 11:35:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/24 11:35:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/24 11:35:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/24 11:35:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extraindo tabela da Bronze\n",
    "order_items = spark.read.parquet(\"bronze/order_items/\")\n",
    "\n",
    "# Tratando colunas\n",
    "order_items = tratamento_dados(order_items, [])\n",
    "\n",
    "order_items = (\n",
    "    order_items\n",
    "    .withColumn(\"order_id\", col(\"order_id\").cast(StringType()))\n",
    "    .withColumn(\"order_item_id\", col(\"order_item_id\").cast(IntegerType()))\n",
    "    .withColumn(\"product_id\", col(\"product_id\").cast(StringType()))\n",
    "    .withColumn(\"seller_id\", col(\"seller_id\").cast(StringType()))\n",
    "    .withColumn(\"shipping_limit_date\", col(\"shipping_limit_date\").cast(TimestampType()))\n",
    "    .withColumn(\"price\", col(\"price\").cast(DoubleType()))\n",
    "    .withColumn(\"freight_value\", col(\"freight_value\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# Selecionando colunas necessárias\n",
    "order_items = order_items.select(\"order_id\", \"order_item_id\", \"product_id\", \"seller_id\", \"shipping_limit_date\", \"price\", \"freight_value\")\n",
    "\n",
    "# Remove linhas duplicadas\n",
    "order_items = order_items.dropDuplicates()\n",
    "\n",
    "# Salvando na Camada\n",
    "order_items.write.mode(\"overwrite\").parquet(f\"{camada_destino}/order_items/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando tabela Order Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extraindo tabela da Bronze\n",
    "order_payments = spark.read.parquet(\"bronze/order_payments/\")\n",
    "\n",
    "# Tratando colunas\n",
    "order_payments = tratamento_dados(order_payments, [\"payment_type\"])\n",
    "\n",
    "order_payments = (\n",
    "    order_payments\n",
    "    .withColumn(\"order_id\", col(\"order_id\").cast(StringType()))\n",
    "    .withColumn(\"payment_sequential\", col(\"payment_sequential\").cast(IntegerType()))\n",
    "    .withColumn(\"payment_type\", col(\"payment_type\").cast(StringType()))\n",
    "    .withColumn(\"payment_installments\", col(\"payment_installments\").cast(IntegerType()))\n",
    "    .withColumn(\"payment_value\", col(\"payment_value\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# Selecionando colunas necessárias\n",
    "order_payments = order_payments.select(\"order_id\", \"payment_sequential\", \"payment_type\", \"payment_installments\", \"payment_value\")\n",
    "\n",
    "# Remove linhas duplicadas\n",
    "order_payments = order_payments.dropDuplicates()\n",
    "\n",
    "# Salvando na Camada\n",
    "order_payments.write.mode(\"overwrite\").parquet(f\"{camada_destino}/order_payments/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando tabela Order Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/24 11:35:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/24 11:35:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/24 11:35:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/24 11:35:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/24 11:35:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extraindo tabela da Bronze\n",
    "order_reviews = spark.read.parquet(\"bronze/order_reviews/\")\n",
    "\n",
    "# Tratando colunas\n",
    "order_reviews = tratamento_dados(order_reviews, [\"review_comment_title\", \"review_comment_message\"])\n",
    "\n",
    "order_reviews = (\n",
    "    order_reviews\n",
    "    .withColumn(\"order_id\", col(\"order_id\").cast(StringType()))\n",
    "    .withColumn(\"review_id\", col(\"review_id\").cast(StringType()))\n",
    "    .withColumn(\"review_score\", col(\"review_score\").cast(IntegerType()))\n",
    "    .withColumn(\"review_comment_title\", col(\"review_comment_title\").cast(StringType()))\n",
    "    .withColumn(\"review_comment_message\", col(\"review_comment_message\").cast(StringType()))\n",
    "    .withColumn(\"review_creation_date\", col(\"review_creation_date\").cast(TimestampType()))\n",
    "    .withColumn(\"review_answer_timestamp\", col(\"review_answer_timestamp\").cast(TimestampType()))\n",
    ")\n",
    "\n",
    "# Selecionando colunas necessárias\n",
    "order_reviews = order_reviews.select(\"order_id\", \"review_id\", \"review_score\", \"review_comment_title\", \"review_comment_message\", \"review_creation_date\", \"review_answer_timestamp\")\n",
    "\n",
    "# Remove linhas duplicadas\n",
    "order_reviews = order_reviews.dropDuplicates()\n",
    "\n",
    "# Salvando na Camada\n",
    "order_reviews.write.mode(\"overwrite\").parquet(f\"{camada_destino}/order_reviews/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando tabela Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/24 11:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/24 11:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/24 11:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/24 11:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/24 11:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extraindo tabela da Bronze\n",
    "orders = spark.read.parquet(\"bronze/orders/\")\n",
    "\n",
    "# Tratando colunas\n",
    "orders = tratamento_dados(orders, [\"order_status\"])\n",
    "\n",
    "orders = (\n",
    "    orders\n",
    "    .withColumn(\"order_id\", col(\"order_id\").cast(StringType()))\n",
    "    .withColumn(\"customer_id\", col(\"customer_id\").cast(StringType()))\n",
    "    .withColumn(\"order_status\", col(\"order_status\").cast(StringType()))\n",
    "    .withColumn(\"order_purchase_timestamp\", col(\"order_purchase_timestamp\").cast(TimestampType()))\n",
    "    .withColumn(\"order_approved_at\", col(\"order_approved_at\").cast(TimestampType()))\n",
    "    .withColumn(\"order_delivered_carrier_date\", col(\"order_delivered_carrier_date\").cast(TimestampType()))\n",
    "    .withColumn(\"order_delivered_customer_date\", col(\"order_delivered_customer_date\").cast(TimestampType()))\n",
    "    .withColumn(\"order_estimated_delivery_date\", col(\"order_estimated_delivery_date\").cast(TimestampType()))\n",
    ")\n",
    "\n",
    "# Selecionando colunas necessárias\n",
    "orders = orders.select(\"order_id\", \"customer_id\", \"order_status\", \"order_purchase_timestamp\", \"order_approved_at\", \"order_delivered_carrier_date\", \"order_delivered_customer_date\", \"order_estimated_delivery_date\")\n",
    "\n",
    "# Remove linhas duplicadas\n",
    "orders = orders.dropDuplicates()\n",
    "\n",
    "# Salvando na Camada\n",
    "orders.write.mode(\"overwrite\").parquet(f\"{camada_destino}/orders/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando tabela Product Category Name Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraindo tabela da Bronze\n",
    "product_category_name_translation = spark.read.parquet(\"bronze/product_category_name_translation/\")\n",
    "\n",
    "# Tratando colunas\n",
    "product_category_name_translation = tratamento_dados(product_category_name_translation, [\"product_category_name\", \"product_category_name_english\"])\n",
    "\n",
    "product_category_name_translation = (\n",
    "    product_category_name_translation\n",
    "    .withColumn(\"product_category_name\", col(\"product_category_name\").cast(StringType()))\n",
    "    .withColumn(\"product_category_name_english\", col(\"product_category_name_english\").cast(StringType()))\n",
    ")\n",
    "\n",
    "# Selecionando colunas necessárias\n",
    "product_category_name_translation = product_category_name_translation.select(\"product_category_name\", \"product_category_name_english\")\n",
    "\n",
    "# Remove linhas duplicadas\n",
    "product_category_name_translation = product_category_name_translation.dropDuplicates()\n",
    "\n",
    "# Salvando na Camada\n",
    "product_category_name_translation.write.mode(\"overwrite\").parquet(f\"{camada_destino}/product_category_name_translation/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando tabela Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extraindo tabela da Bronze\n",
    "products = spark.read.parquet(\"bronze/products/\")\n",
    "\n",
    "# Tratando colunas\n",
    "products = tratamento_dados(products, [\"product_category_name\"])\n",
    "\n",
    "# Realizando o cast das colunas\n",
    "products = (\n",
    "    products\n",
    "    .withColumn(\"product_id\", col(\"product_id\").cast(StringType()))  \n",
    "    .withColumn(\"product_category_name\", col(\"product_category_name\").cast(StringType()))\n",
    "    .withColumn(\"product_name_lenght\", col(\"product_name_lenght\").cast(IntegerType()))\n",
    "    .withColumn(\"product_description_lenght\", col(\"product_description_lenght\").cast(IntegerType()))\n",
    "    .withColumn(\"product_photos_qty\", col(\"product_photos_qty\").cast(IntegerType()))\n",
    "    .withColumn(\"product_weight_g\", col(\"product_weight_g\").cast(IntegerType()))\n",
    "    .withColumn(\"product_length_cm\", col(\"product_length_cm\").cast(IntegerType()))\n",
    "    .withColumn(\"product_height_cm\", col(\"product_height_cm\").cast(IntegerType()))\n",
    "    .withColumn(\"product_width_cm\", col(\"product_width_cm\").cast(IntegerType()))\n",
    ")\n",
    "\n",
    "# Selecionando as colunas necessárias\n",
    "products = products.select(\n",
    "    \"product_id\",\n",
    "    \"product_category_name\",\n",
    "    \"product_name_lenght\",\n",
    "    \"product_description_lenght\",\n",
    "    \"product_photos_qty\",\n",
    "    \"product_weight_g\",\n",
    "    \"product_length_cm\",\n",
    "    \"product_height_cm\",\n",
    "    \"product_width_cm\"\n",
    ")\n",
    "\n",
    "# Remove linhas duplicadas\n",
    "products = products.dropDuplicates()\n",
    "\n",
    "# Salvando na Camada\n",
    "products.write.mode(\"overwrite\").parquet(f\"{camada_destino}/products/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando tabela sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraindo tabela da Bronze\n",
    "sellers = spark.read.parquet(\"bronze/sellers/\")\n",
    "\n",
    "# Tratando colunas\n",
    "sellers = tratamento_dados(sellers, [\"seller_city\", \"seller_state\"])\n",
    "\n",
    "# Aplicando a função para tratar o zip code\n",
    "sellers = tratar_zip_code(sellers, \"seller_zip_code_prefix\")\n",
    "\n",
    "# Realizando o cast das colunas\n",
    "sellers = (\n",
    "    sellers\n",
    "    .withColumn(\"seller_id\", col(\"seller_id\").cast(StringType()))  \n",
    "    .withColumn(\"seller_zip_code_prefix\", col(\"seller_zip_code_prefix\").cast(StringType()))\n",
    "    .withColumn(\"seller_city\", col(\"seller_city\").cast(StringType()))\n",
    "    .withColumn(\"seller_state\", col(\"seller_state\").cast(StringType()))\n",
    ")\n",
    "\n",
    "# Selecionando as colunas necessárias\n",
    "sellers = sellers.select(\n",
    "    \"seller_id\",\n",
    "    \"seller_zip_code_prefix\",\n",
    "    \"seller_city\",\n",
    "    \"seller_state\",\n",
    ")\n",
    "\n",
    "# Remove linhas duplicadas\n",
    "sellers = sellers.dropDuplicates()\n",
    "\n",
    "# Salvando na Camada\n",
    "sellers.write.mode(\"overwrite\").parquet(f\"{camada_destino}/sellers/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
